{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a31fcd9",
   "metadata": {},
   "source": [
    "## FAIRNESS - NEW DEFINTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130bf3e",
   "metadata": {},
   "source": [
    "We have seen several popular [definitions of fairness](Fair-Classifier.ipynb) and their [implementation](Fairness-Definitions-Comparison.ipynb). Here we will try to frame some new definitions for measuring fairness of data and fairness of trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054073b9",
   "metadata": {},
   "source": [
    "### PROTECTED INFO LEAK TEST "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3427250",
   "metadata": {},
   "source": [
    "Here we will try to assess the fairness of the model, using the amount of protected group membership information leaked by the model.\n",
    "\n",
    "#### **DEFINITION(PROTECTED INFO LEAK TEST  a.k.a PIL TEST)**\n",
    "Given a classification model(call it $M$, trained on protected variable and other features to predict class labels) and large test data, train two other models:\n",
    "- Classification model(call it $M'$) trained on predicted class labels by $M$ and other features of the test data, except actual class labels to predict protected group membership.\n",
    "- Classification model(call it $M''$) trained on all features of the test data, except the actual class labels and  to predict protected group membership.\n",
    "\n",
    "Fix a threshold percentage(say $th\\%$). Let the predicted protected group membership by $M'$ has an accuracy $A_{M'}$ and $M''$ has an accuracy $A_{M''}$. If $\\frac{A_{M'}-A_{M''}}{100-A_{M'}}\\times 100\\geq th\\%$, we will deem the classifier $M$ as unfair. $\\blacksquare$\n",
    "\n",
    "$\\frac{A_{M'}-A_{M''}}{100-A_{M'}}\\times 100$ in called **protected info leak(PIL) value**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ca919",
   "metadata": {},
   "source": [
    "#### **DISCUSSION**\n",
    "\n",
    "- Why such a definition?\n",
    "    - If the model $M$ predicts such labels that there isn't a considerable increase in our accuracy of predicting back the protected group membership using predicted class label of the individual, then we will have low PIL value. This means our model doesn't leak protected group information while generating predictions. That means it acts fairly.\n",
    "    \n",
    "\n",
    "- Our definition provides flexibility of using any classification models $M'$ and $M''$, ideally $M'$ and $M''$ should be using same classification algorithms i.e. either both Logistic Regression, SVM, etc.\n",
    "\n",
    "\n",
    "- Significance of threshold, $th$?\n",
    "    - In many situations, protected group membership provides valuable information to produce accurate predictions. So, the model $M$ may not completely neglect the contribution of protected group. So, naturally we would be able to predict the protected group membership using $M'$ better as it contains the predicted class labels which in turn depends on information provided by protected group variable. In that case, we must set a threshold value for upto how much we will consider the increase in prediction accuracy of $M'$ over $M''$ as not a violation to fairness. \n",
    "    \n",
    "- Why such an expression for PIL value?\n",
    "    - We see that the denominator is the amount of accuracy left by the model with all features except using predictions from model $M$ and the numerator is the increase in the accuracy of predicting back protected group membership using the predictions of model $M$.\n",
    "    \n",
    "- What is the range of the PIL value and what does it signify?\n",
    "    - The range of PIL value is between $0\\%$ and $100\\%$, a higher PIL value means higher levels of unfairness i.e. we may consider $>50\\%$ as unfair. For some other domain where we are aware that protected variables provide useful information for predictions then we can set the $th\\%$ higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64376d55",
   "metadata": {},
   "source": [
    "#### IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b77113",
   "metadata": {},
   "source": [
    "The above definition is used to compare two models trained by [regularization based on fairness](Fairness-Definitions-Comparison.ipynb) and [without fairness regularization](Fairness-Regularization.ipynb). The [PIL Test implementation](PIL-Test-Implementation.ipynb) can be seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd8396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
