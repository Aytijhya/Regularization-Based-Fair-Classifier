{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7faf53",
   "metadata": {},
   "source": [
    "## PIL TEST - IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1745193e",
   "metadata": {},
   "source": [
    "Here we will see the implementation of the [PIL Test](Fairness-New-Definitions.ipynb) on the [Adult Dataset](https://archive.ics.uci.edu/ml/datasets/adult)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b87399",
   "metadata": {},
   "source": [
    "### IMPORTNG THE NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e76a2f",
   "metadata": {},
   "source": [
    "We will be using the open-source [AIF360](https://github.com/Trusted-AI/AIF360) package to use several fairness based metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0fe4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress = True)\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the Dataset\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131cfd6",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52023b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "priv_group = [{'sex':1}]\n",
    "unpriv_group = [{'sex':0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3857bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adult = load_preproc_data_adult(['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efce573",
   "metadata": {},
   "source": [
    "### LOADING PRE-TRAINED MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1bf2f",
   "metadata": {},
   "source": [
    "We trained two models simple logistic regression and equalized odds regularized logistic regression [here](Fairness-Regularization.ipynb). So, we load these models for testing our new definition of fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "515d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_Reg(nn.Module):\n",
    "    def __init__(self, size_in):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(size_in, 1)\n",
    "    def forward(self, x):\n",
    "        prob_pred = torch.sigmoid(self.linear(x))\n",
    "        return prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d2b0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw-test-data.bin\", \"rb\") as input:\n",
    "    dset_raw_tst = pickle.load(input)\n",
    "    \n",
    "with open(\"trained-std-scaler.bin\", \"rb\") as input:\n",
    "    scaler = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5aad3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Log_Reg(len(dset_raw_tst.feature_names)) # Non-Fairness Based Regularized Model\n",
    "M_F = Log_Reg(len(dset_raw_tst.feature_names)) # Fairness Based Regularization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d812fdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Log_Reg(\n",
       "  (linear): Linear(in_features=18, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.load_state_dict(torch.load(\"simple-logistic-regression.pt\"))\n",
    "M_F.load_state_dict(torch.load(\"equalized-odd-regualarized-logistic-regression.pt\"))\n",
    "\n",
    "M.eval()\n",
    "M_F.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974adb9",
   "metadata": {},
   "source": [
    "### PREDICTIONS OF THE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260adc55",
   "metadata": {},
   "source": [
    "Here we get the predicted values of both the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0c6f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_tst_pred_M = dset_raw_tst.copy(deepcopy=True)\n",
    "dset_tst_pred_M_F = dset_raw_tst.copy(deepcopy=True)\n",
    "dset_tst = scaler.transform(dset_tst_pred.features)\n",
    "dset_tst_pred_M.labels = (M(torch.from_numpy(dset_tst).float()) > 0.5).numpy().astype(float)\n",
    "dset_tst_pred_M_F.labels = (M_F(torch.from_numpy(dset_tst).float()) > 0.5).numpy().astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a726e22",
   "metadata": {},
   "source": [
    "We now prepare the data for training the 'adversarial' models for predicting the protected group membership from the remaining features of the test data (and the predicted labels of the original model), as per the [PIL Test](Fairness-New-Definitions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e95aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dset_tst_pred_M.features[:,1].reshape((-1, 1))\n",
    "# replacing the gender column with model M's predicted values\n",
    "dset_tst_pred_M.features[:,1] = dset_tst_pred_M.labels.ravel()\n",
    "# replacing the gender column with model M_F's predicted values\n",
    "dset_tst_pred_M_F.features[:,1] = dset_tst_pred_M_F.labels.ravel()\n",
    "\n",
    "dset_other_features = dset_raw_tst.features[:,[0]+list(range(2,len(dset_tst_pred_M.feature_names)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6716d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_M = StandardScaler()\n",
    "dset_tst_M = scaler_M.fit_transform(dset_tst_pred_M.features)\n",
    "\n",
    "scaler_M_F = StandardScaler()\n",
    "dset_tst_M_F = scaler_M_F.fit_transform(dset_tst_pred_M_F.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "faf308c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_tst_M = torch.from_numpy(dset_tst_M).float()\n",
    "dset_tst_M_F = torch.from_numpy(dset_tst_M_F).float()\n",
    "dset_other_features = torch.from_numpy(dset_other_features).float()\n",
    "y = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10ff0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_mod_pred_M = Log_Reg(len(dset_tst_pred_M.feature_names))\n",
    "adv_mod_wo_pred = Log_Reg((len(dset_tst_pred_M.feature_names)-1))\n",
    "adv_mod_pred_M_F = Log_Reg(len(dset_tst_pred_M_F.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "faeb312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000 # Number of Epochs\n",
    "learning_rate = 0.01 # Learning Rate\n",
    "\n",
    "# Stochastic Gradient Descent Optimizers\n",
    "optimizer_M = torch.optim.SGD(adv_mod_pred_M.parameters(), lr= learning_rate)\n",
    "optimizer_M_F = torch.optim.SGD(adv_mod_pred_M_F.parameters(), lr= learning_rate)\n",
    "optimizer_wo_pred = torch.optim.SGD(adv_mod_wo_pred.parameters(), lr= learning_rate)\n",
    "\n",
    "# Binary Cross Entropy Loss Functions\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5edd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training adv_mod_wo_pred:\n",
      "epoch: 500, loss = 0.3487\n",
      "epoch: 1000, loss = 0.2993\n",
      "epoch: 1500, loss = 0.2686\n",
      "epoch: 2000, loss = 0.2466\n",
      "epoch: 2500, loss = 0.2302\n",
      "epoch: 3000, loss = 0.2177\n",
      "epoch: 3500, loss = 0.2079\n",
      "epoch: 4000, loss = 0.1999\n",
      "epoch: 4500, loss = 0.1933\n",
      "epoch: 5000, loss = 0.1878\n",
      "Trained adv_mod_wo_pred's Performance on Data:\n",
      "Accuracy:  91.34648194908893\n"
     ]
    }
   ],
   "source": [
    "# Training adv_mod_wo_pred\n",
    "print(\"Training adv_mod_wo_pred:\")\n",
    "adv_mod_wo_pred.train()\n",
    "for epoch in range(num_epochs):\n",
    "    p_pred = adv_mod_wo_pred(dset_other_features)\n",
    "    loss= criterion(p_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_wo_pred.step()\n",
    "    \n",
    "    optimizer_wo_pred.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 500== 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "print(\"Trained adv_mod_wo_pred's Performance on Data:\")        \n",
    "with torch.no_grad():\n",
    "    y_pred = (p_pred > 0.5).numpy().astype(float)\n",
    "    accuracy = (np.sum(np.sum(np.array(y_pred.ravel()) == np.array(y.ravel())))/len(y.ravel()))*100\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ea22421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training adv_mod_pred_M:\n",
      "epoch: 500, loss = 0.2004\n",
      "epoch: 1000, loss = 0.1083\n",
      "epoch: 1500, loss = 0.0729\n",
      "epoch: 2000, loss = 0.0545\n",
      "epoch: 2500, loss = 0.0434\n",
      "epoch: 3000, loss = 0.0360\n",
      "epoch: 3500, loss = 0.0307\n",
      "epoch: 4000, loss = 0.0268\n",
      "epoch: 4500, loss = 0.0237\n",
      "epoch: 5000, loss = 0.0213\n",
      "Trained adv_mod_pred_M's Performance on Data:\n",
      "Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "# Training adv_mod_pred_M\n",
    "print(\"Training adv_mod_pred_M:\")\n",
    "adv_mod_pred_M.train()\n",
    "for epoch in range(num_epochs):\n",
    "    p_pred = adv_mod_pred_M(dset_tst_M)\n",
    "    loss= criterion(p_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_M.step()\n",
    "    \n",
    "    optimizer_M.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 500== 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "print(\"Trained adv_mod_pred_M's Performance on Data:\")        \n",
    "with torch.no_grad():\n",
    "    y_pred = (p_pred > 0.5).numpy().astype(float)\n",
    "    accuracy = (np.sum(np.sum(np.array(y_pred.ravel()) == np.array(y.ravel())))/len(y.ravel()))*100\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "981a6fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training adv_mod_pred_M_F:\n",
      "epoch: 500, loss = 0.1881\n",
      "epoch: 1000, loss = 0.1881\n",
      "epoch: 1500, loss = 0.1881\n",
      "epoch: 2000, loss = 0.1881\n",
      "epoch: 2500, loss = 0.1881\n",
      "epoch: 3000, loss = 0.1881\n",
      "epoch: 3500, loss = 0.1881\n",
      "epoch: 4000, loss = 0.1881\n",
      "epoch: 4500, loss = 0.1881\n",
      "epoch: 5000, loss = 0.1881\n",
      "Trained adv_mod_pred_M_F's Performance on Data:\n",
      "Accuracy:  94.91571691803726\n"
     ]
    }
   ],
   "source": [
    "# Training adv_mod_pred_M\n",
    "print(\"Training adv_mod_pred_M_F:\")\n",
    "adv_mod_pred_M_F.train()\n",
    "for epoch in range(num_epochs):\n",
    "    p_pred = adv_mod_pred_M(dset_tst_M_F)\n",
    "    loss= criterion(p_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_M_F.step()\n",
    "    \n",
    "    optimizer_M_F.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 500== 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "print(\"Trained adv_mod_pred_M_F's Performance on Data:\")        \n",
    "with torch.no_grad():\n",
    "    y_pred = (p_pred > 0.5).numpy().astype(float)\n",
    "    accuracy = (np.sum(np.sum(np.array(y_pred.ravel()) == np.array(y.ravel())))/len(y.ravel()))*100\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba1925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
