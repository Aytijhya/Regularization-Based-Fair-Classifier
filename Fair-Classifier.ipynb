{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5048ea37",
   "metadata": {},
   "source": [
    "# Fair Classifier - Defintions & Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ceec87",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00bdc95",
   "metadata": {},
   "source": [
    "With the increasing dependence on Machine Learning Tools and techniques in our daily life, it becomes increasing important to address the problem of **fairness** of our trained model. So, that it can take a fair decision on real-life situations. Afterall, we train models to reduce human biasness in critical automated data-driven decision making situations.\n",
    "\n",
    "Here, I will look at the several definitions of fairness(in context of machine learning) and will look at techniques and methods to enforce fairness in the trained model out of the data. I will also present new definitions of fairness and new methods to mitigate the fairness based bias of the model, along with their implementations and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3e476",
   "metadata": {},
   "source": [
    "**Fairness** is an idea of how two or more different groups should be treated so that the decision taken given a feature vector is free from the influence of the group membership thereby taking a 'just' decision. But the justification of fairness and how it should be measured is still quite domain specific and has its roots in philosophical and ethical questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882eb70",
   "metadata": {},
   "source": [
    "#### NOTATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce59a4",
   "metadata": {},
   "source": [
    "In the following discussion, we will use the following terminology:\n",
    "- **G**: Protected or sensitive attribute for which non-discrimination should be established. We assume the groups under G are m and f of which f is underprivileged and m is privileged.\n",
    "- **X**: All additional attributes describing the individual.\n",
    "- **Y**: The actual classification result\n",
    "- **S**: Predicted probability for a certain classification c, P(Y = c |G,X)\n",
    "- **d**: Predicted decision (category) for the individual; d is usually derived from S, e.g., d = 1 when S is above a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee93e9f",
   "metadata": {},
   "source": [
    "### FAIRNESS METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72daa5d",
   "metadata": {},
   "source": [
    "Some statistical metrics which can used to measure the Fairness observed on a given data. There is no specific rule of why one metric is better than all other metrics, the metric is chosen depending on the area where the developed model trained on the data or the data itself will be used and the definition of fairness used in that context. In most cases we are interested in establishing fairness for Binary Classification or Binary G. The metrics used are stated below:\n",
    "1. **True positive (TP)**: Number of cases when the predicted and actual outcomes are both in the positive class.\n",
    "2. **False positive (FP)**: Number of cases when predicted to be in the positive class when the actual outcome belongs to the negative class.\n",
    "3. **False negative (FN)**: Number of cases predicted to be in the negative class when the actual outcome belongs to the positive class.\n",
    "4. **True negative (TN)**: Number of cases when the predicted and actual outcomes are both in the negative class.\n",
    "5. **Positive predictive value (PPV)**: PPV is $\\frac{TP}{TP+FP}$, the empirical value of $P(Y=1|d=1)$ calculated from the data.\n",
    "6. **False discovery rate (FDR)**: FDR is $\\frac{FP}{TP+FP}$, the empirical value of $P(Y=0|d=1)$ calculated from the data.\n",
    "7. **False omission rate (FOR)**: FOR is $\\frac{FN}{TN+FN}$, the empirical value of $P(Y=1|d=0)$ calculated from the data. \n",
    "8. **Negative predictive value (NPV)**: NPV is $\\frac{TN}{TN+FN}$, the empirical value of $P(Y=0|d=0)$ calculated from the data. \n",
    "9. **True positive rate (TPR)**: TPR is $\\frac{TP}{TP+FN}$, the empirical value of $P(d=1|Y=1)$ calculated from the data.\n",
    "10. **False positive rate (FPR)**: FPR is $\\frac{FP}{TN+FP}$, the empirical value of $P(d=1|Y=0)$ calculated from the data.\n",
    "11. **False negative rate (FNR)**: FNR is $\\frac{FN}{TP+FN}$, the empirical value of $P(d=0|Y=1)$ calculated from the data.\n",
    "12. **True negative rate (TNR)**: TNR is $\\frac{TN}{TN+FP}$, the empirical value of $P(d=0|Y=0)$ calculated from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bf54d",
   "metadata": {},
   "source": [
    "### FAIRNESS DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342de3d6",
   "metadata": {},
   "source": [
    "Some popular definitions of fairness of data(*irrespective of the model*) are stated below:\n",
    "- **Statistical Parity Difference**: Statistical Parity Difference close to 0 shows our data is fair(in terms of this definition, in the following definition by 'fair' we will refer to that particular definition). If this value is far from 0 then our data exhibits lack of fairness. It is calculated as the empirical version of $$P(Y=1|G=f)-P(Y=1|G=m)$$ A modified version of this is letting the consideration that some legitimate factor can affect the probabilities.\n",
    "- **Disparate Impact**: Disparate Impact close to 1 shows our data is fair. If this value if below some particular threshold, we suspect unfairness. It is calculated as the empirical version of $$\\frac{P(Y=1|G=f)}{P(Y=1|G=m)}$$\n",
    "- **Consistency**: This unlike the other fairness metric is an *individual fairness metric* i.e. how similar are the outcomes for similar observed values for different individuals. It is calculated by $$1−\\frac{1}{n}\\sum_{i=1}^n|Y_i−\\frac{1}{n\\_neighbors}\\sum_{j\\in N_{n\\_neighbors(x_i)}}Y_j|$$ where we need to choose the number of neighbors we want to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe27a1",
   "metadata": {},
   "source": [
    "Some popular definitions of fairness of Model are stated below:\n",
    "- **Predictive Parity Difference**: Predictive Parity Difference close to 0 shows our model is fair. If this value is far from 0 then our model exhibits lack of fairness. It is calculated as the empirical version of $$PPV(G=f)-PPV(G=m)$$\n",
    "- **FPR Difference**: The name implies what this definition is. It is calculated as the empirical version of $$FPR(G=f)-FPR(G=m)$$\n",
    "- **FNR Difference**: The name implies what this definition is. It is calculated as the empirical version of $$FNR(G=f)-FNR(G=m)$$\n",
    "- **Equalized Odds**: This considers the above two definitions of FNR and FPR Difference and if they both are close to 0 then we say that our model is fair. Otherwise unfair.\n",
    "- **Accuracy Difference**: The name implies what this definition is. It is calculated as the empirical version of $$P(d=Y,G=f) - P(d=Y,G=m)$$\n",
    "- **Error Ratio Difference**: Error Rate Difference close to 0 shows our model is fair. If this value is far from 0 then our model exhibits lack of fairness. It is calculated as the empirical version of $$\\frac{FN(G=f)}{FP(G=f)}-\\frac{FN(G=m)}{FP(G=m)}$$\n",
    "- **Calibration Difference**: For some partition for S in $[0,1]$, in each partition the Calibration Difference should be close to 0. It is calculated as the empirical version of $$P(Y=1|S=s,G=f)-P(Y=1|S=s,G=m),\\ \\forall s\\in\\ partition$$ An extension of this above also enforces that the individual probabilities whose difference is taken should be s.\n",
    "- **Average Positive Class Difference**: The name implies what this definition is. It is calculated as the empirical version of $$E(S|Y=1,G=f)-E(S|Y=1,G=m)$$\n",
    "- **Average Negative Class Difference**: The name implies what this definition is. It is calculated as the empirical version of $$E(S|Y=0,G=f)-E(S|Y=0,G=m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba161298",
   "metadata": {},
   "source": [
    "#### IMPLEMENTATION & COMPARISON\n",
    "I have implemented and compared the results of the above definitions on the Adult Dataset in the following notebook.\n",
    "\n",
    "[Notebook for comparison between the above mentioned definitions implementation on the Adult Dataset](Fairness-Definitions-Comparison.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bc62a",
   "metadata": {},
   "source": [
    "#### NEW DEFINITIONS\n",
    "I have framed some definitions concerning the assessment of fairness of a model. They are presented in the following notebook.\n",
    "\n",
    "[Notebook for new definitions of fairness, their discussions and implementation](Fairness-New-Definitions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5efa3",
   "metadata": {},
   "source": [
    "### FAIRNESS PROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58277ff8",
   "metadata": {},
   "source": [
    "Fairness can affect three stages of data modeling pipelines. The stages are mentioned below:\n",
    "- **Fairness Pre-Processing**: This is the earliest stage of enforcing fairness across the dataset before we proceed with training a model using this data. It is the most flexible fairness intervention. Some methods for fairness pre-processing are-\n",
    "    - ***Suppression***\n",
    "    - ***Relabling*** \n",
    "    - ***Reweighting***\n",
    "    - ***Learning Fair Representations***\n",
    "- **Fairness In-Processing**: This is the fairness intervention during the process of training a model. Some methods for fairness in-processing are-\n",
    "    - [***Regularization***](Fairness-Regularization.ipynb)\n",
    "    - ***Adversarial Debiasing***\n",
    "- **Fairness Post-Processing**: This is the fairness intervention which comes at the last stage when the data has already been pre-processed and the model is trained. This is the last resort to improve on fairness of the model performance when the above two steps weren't or can't be done incorporating fairness. Some methods for fairness post-processing are-\n",
    "    - ***Equality of Oppurtunity***\n",
    "    - ***Calibration-Preserving Equalized Odds***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56caf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Rishi Dey Chowdhury"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "title": "Fair Classifier"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
